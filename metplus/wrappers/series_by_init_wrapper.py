'''! @namespace SeriesByInitWrapper
@brief Performs any optional filtering of input tcst data then performs
regridding via the MET tool regrid_data_plane, then builds up
the commands to perform a series analysis by init time by invoking the
MET tool series_analysis. NetCDF plots are generated by invoking the MET tool
plot_data_plane. The NetCDF plots are then converted to .png and Postscript.

Call as follows:
@code{.sh}
series_by_init.py [-c /path/to/user.template.conf]
@endcode
'''

import errno
import os
import re
import sys
from datetime import datetime

from ..util import met_util as util
from ..util import ti_calculate, do_string_sub
from ..util import get_lead_sequence, get_lead_sequence_groups
from ..util import ti_get_hours_from_lead
from .plot_data_plane_wrapper import PlotDataPlaneWrapper
from . import RuntimeFreqWrapper

class SeriesByInitWrapper(RuntimeFreqWrapper):
    """!  Performs series analysis based on init time by first performing any
          additional filtering via the wrapper to the MET tool tc_stat,
          tc_stat_wrapper.  Next, the arguments to run the MET tool
          series_analysis is done
    """
    # class variables to define prefixes for intermediate files
    FCST_ASCII_FILE_PREFIX = 'FCST_FILES'
    ANLY_ASCII_FILE_PREFIX = 'ANLY_FILES'

    def __init__(self, config, instance=None, config_overrides={}):
        self.app_name = 'series_analysis'
        self.app_path = os.path.join(config.getdir('MET_BIN_DIR', ''),
                                     self.app_name)

        super().__init__(config,
                         instance=instance,
                         config_overrides=config_overrides)

        # override log name
        self.log_name = 'series_by_init'

        self.plot_data_plane = self.plot_data_plane_init()

        self.logger.debug("Initialized SeriesByInitWrapper")

    def create_c_dict(self):
        c_dict = super().create_c_dict()
        c_dict['MODEL'] = self.config.getstr('config',
                                             'MODEL',
                                             'FCST')
        c_dict['REGRID_TO_GRID'] = (
            self.config.getstr('config',
                               'SERIES_ANALYSIS_REGRID_TO_GRID',
                               '')
        )

        # get stat list to loop over
        c_dict['STAT_LIST'] = util.getlist(
            self.config.getstr('config',
                               'SERIES_ANALYSIS_STAT_LIST',
                               '')
        )
        if not c_dict['STAT_LIST']:
            self.log_error("Must set SERIES_ANALYSIS_STAT_LIST to run.")

        # set stat list to set output_stats.cnt in MET config file
        self.set_c_dict_list(c_dict,
                             'SERIES_ANALYSIS_STAT_LIST',
                             'cnt',
                             'OUTPUT_STATS_CNT')

        c_dict['TILE_INPUT_DIR'] = self.config.getdir('SERIES_ANALYSIS_TILE_INPUT_DIR',
                                                 '')
        if not c_dict['TILE_INPUT_DIR']:
            self.log_error("Must set SERIES_ANALYSIS_TILE_INPUT_DIR")

        # set fcst and obs input dir to tile input dir to help find files
        c_dict['FCST_INPUT_DIR'] = c_dict['TILE_INPUT_DIR']
        c_dict['OBS_INPUT_DIR'] = c_dict['TILE_INPUT_DIR']

        c_dict['FCST_INPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'FCST_SERIES_ANALYSIS_TILE_INPUT_TEMPLATE')
        )
        c_dict['OBS_INPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'OBS_SERIES_ANALYSIS_TILE_INPUT_TEMPLATE')
        )

        c_dict['TC_STAT_INPUT_DIR'] = (
            self.config.getdir('SERIES_ANALYSIS_TC_STAT_INPUT_DIR', '')
        )

        c_dict['TC_STAT_INPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'SERIES_ANALYSIS_TC_STAT_INPUT_TEMPLATE')
        )

        c_dict['OUTPUT_DIR'] = self.config.getdir('SERIES_ANALYSIS_OUTPUT_DIR',
                                                  '')
        c_dict['OUTPUT_TEMPLATE'] = (
            self.config.getraw('config',
                               'SERIES_ANALYSIS_OUTPUT_TEMPLATE')
        )
        if not c_dict['OUTPUT_DIR']:
            self.log_error("Must set SERIES_ANALYSIS_OUTPUT_DIR to run.")

        c_dict['FILTERED_OUTPUT_DIR'] = (
            self.config.getdir('SERIES_ANALYSIS_FILTERED_OUTPUT_DIR',
                               '')
        )

        c_dict['TC_STAT_OUTPUT_TEMPLATE'] = self.config.getraw('config',
                                                               'TC_STAT_OUTPUT_TEMPLATE')

        c_dict['FCST_TILE_PREFIX'] = self.config.getstr('config',
                                              'FCST_EXTRACT_TILES_PREFIX',
                                              '')
        if not c_dict['FCST_TILE_PREFIX']:
            self.log_error("Must set FCST_EXTRACT_TILES_PREFIX")

        c_dict['ANLY_TILE_PREFIX'] = self.config.getstr('config',
                                              'OBS_EXTRACT_TILES_PREFIX',
                                              '')
        if not c_dict['ANLY_TILE_PREFIX']:
            self.log_error("Must set OBS_EXTRACT_TILES_PREFIX")

        c_dict['FCST_TILE_REGEX'] = (
            f".*{c_dict['FCST_TILE_PREFIX']}.*nc"
        )

        c_dict['ANLY_TILE_REGEX'] = (
            f".*{c_dict['ANLY_TILE_PREFIX']}.*nc"
        )

        c_dict['CONFIG_FILE'] = self.config.getstr('config',
                                                   'SERIES_ANALYSIS_CONFIG_FILE',
                                                   '')
        if not c_dict['CONFIG_FILE']:
            self.log_error("SERIES_ANALYSIS_CONFIG_FILE must be set")

        c_dict['BACKGROUND_MAP'] = self.config.getbool('config',
                                             'SERIES_ANALYSIS_BACKGROUND_MAP',
                                             False)

        c_dict['VAR_LIST'] = util.parse_var_list(self.config)
        if not c_dict['VAR_LIST']:
            self.log_error("No fields specified. Please set "
                           "[FCST/OBS]_VAR<n>_[NAME/LEVELS]")

        c_dict['GENERATE_PLOTS'] = (
            self.config.getbool('config',
                                'SERIES_ANALYSIS_GENERATE_PLOTS',
                                True)
        )

        c_dict['RUN_ONCE_PER_STORM_ID'] = (
            self.config.getbool('config',
                                'SERIES_ANALYSIS_RUN_ONCE_PER_STORM_ID',
                                True)
        )
        if (c_dict['RUN_ONCE_PER_STORM_ID'] and
                not c_dict['TC_STAT_INPUT_TEMPLATE']):
            self.log_error("Must set SERIES_ANALYSIS_TC_STAT_INPUT_TEMPLATE "
                           "if SERIES_ANALYSIS_RUN_ONCE_PER_STORM_ID is True")

        # if no forecast lead sequence is specified,
        # use wildcard (*) so all leads are used
        c_dict['WILDCARD_LEAD_IF_EMPTY'] = True

        # allow multiple files so wildcards can be used to get input files
        c_dict['ALLOW_MULTIPLE_FILES'] = True

        return c_dict

    def plot_data_plane_init(self):
        # set values to allow successful initialization of PlotDataPlaneWrapper
        plot_overrides = {'PLOT_DATA_PLANE_INPUT_TEMPLATE': 'template',
                          'PLOT_DATA_PLANE_OUTPUT_TEMPLATE': 'template',
                          'PLOT_DATA_PLANE_FIELD_NAME': 'field_name',
                          'PLOT_DATA_PLANE_CONVERT_TO_IMAGE': True,
                          }

        if not self.c_dict['BACKGROUND_MAP']:
            plot_overrides['PLOT_DATA_PLANE_FIELD_EXTRA'] = (
                "map_data={ source=[];}"
            )

        pdp_wrapper = PlotDataPlaneWrapper(self.config,
                                           config_overrides=plot_overrides)
        return pdp_wrapper

    def run_once_per_lead(self):
        self.logger.debug("Running once for forecast lead time")
        success = True

        lead_groups = get_lead_sequence_groups(self.config)
        if not lead_groups:
            lead_seq = get_lead_sequence(self.config,
                                         input_dict=None,
                                         wildcard_if_empty=True)
            for index, lead in enumerate(lead_seq):
                lead_hours = str(ti_get_hours_from_lead(lead)).zfill(3)
                lead_groups[f'series_F{lead_hours}'] = [lead]

        for lead_group in lead_groups.items():
            # create input dict and only set 'now' item
            # create a new dictionary each iteration in case the function
            # that it is passed into modifies it
            input_dict = set_input_dict(loop_time=None,
                                        config=self.config,
                                        use_init=None)

            input_dict['init'] = '*'
            input_dict['valid'] = '*'

            self.logger.debug(f"Processing {lead_group[0]} - forecast leads: "
                              f"{', '.join(lead_group[1])}")
            if not self.run_at_time_once(input_dict, lead_group):
                success = False

        return success

    def run_at_time_once(self, time_info, lead_group=None):
        """! Invoke the series analysis script based on the init time

            @param time_info dictionary containing time information
            @returns True on success, False otherwise
        """
        self.logger.debug("Starting series analysis by init time")

        # if running for each storm ID, get list of storms
        storm_list = self.get_storm_list(time_info)
        if not storm_list:
            return False

        # loop over storm list and process for each
        # this loop will execute once if not filtering by storm ID
        for storm_id in storm_list:
            # Create FCST and ANLY ASCII files based on init time and storm id
            fcst_path, obs_path = (
                self.create_ascii_storm_files_list(time_info,
                                                   storm_id,
                                                   lead_group)
            )
            if not fcst_path or not obs_path:
                self.log_error('No ASCII file lists were created. Skipping.')
                continue

            # Build up the arguments to and then run the MET tool series_analysis.
            if not self.build_and_run_series_request(time_info,
                                                     fcst_path,
                                                     obs_path):
                continue

            if self.c_dict['GENERATE_PLOTS']:
                self.generate_plots(time_info, storm_id)
            else:
                self.logger.debug("Skip plotting output. Change "
                                  "SERIES_ANALYSIS_GENERATE_PLOTS to True to "
                                  "run this step.")

        self.logger.debug("Finished series analysis by init time")
        return True

    def get_storm_list(self, time_info):
        """! Find the .tcst filter file for the current run time and get the
             list of storm IDs that are found in the file.

            @param time_info dictionary containing time information
            @returns A list of all the storms ids that correspond to the
             current init time or None if filter file does not exist
        """
        if not self.c_dict['RUN_ONCE_PER_STORM_ID']:
            return ['*']

        # Retrieve filter files, first create the filename
        # by piecing together the out_dir_base with the cur_init.
        filter_template = os.path.join(self.c_dict['TC_STAT_INPUT_DIR'],
                                       self.c_dict['TC_STAT_INPUT_TEMPLATE'])
        filter_file = do_string_sub(filter_template, **time_info)
        self.logger.debug(f"Getting storms from filter file: {filter_file}")
        if not os.path.exists(filter_file):
            self.log_error(f"Filter file does not exist: {filter_file}")
            return None

        # Now that we have the filter filename for the init time, let's
        # extract all the storm ids in this filter file.
        storm_list = util.get_storm_ids(filter_file, self.logger)
        if not storm_list:
            # No storms for this init time, check next init time in list
            self.logger.debug(f"No storms found for current runtime")
            return None

        return storm_list

    def get_files_from_time(self, time_info):
        """! Create dictionary containing time information (key time_info) and
             any relevant files for that runtime. The parent implementation of
             this function creates a dictionary and adds the time_info to it.
             This wrapper gets all files for the current runtime and adds it to
             the dictionary with keys 'fcst' and 'anly'

             @param time_info dictionary containing time information
             @returns dictionary containing time_info dict and any relevant
             files with a key representing a description of that file
        """
        file_dict_list = []
        # get all storm IDs
        storm_list = self.get_storm_list(time_info)
        if not storm_list:
            return None

        for storm_id in storm_list:
            time_info['storm_id'] = storm_id
            file_dict = super().get_files_from_time(time_info)
            fcst_files = self.find_input_files(time_info, 'FCST')
            anly_files = self.find_input_files(time_info, 'OBS')
            if fcst_files is None or anly_files is None:
                return None

            file_dict['fcst'] = fcst_files
            file_dict['anly'] = anly_files
            file_dict_list.append(file_dict)

        return file_dict_list

    def find_input_files(self, time_info, data_type):
        """! Loop over list of input templates and find files for each

             @param time_info time dictionary to use for string substitution
             @returns Input file list if all files were found, None if not.
        """
        input_files = self.find_data(time_info,
                                     return_list=True,
                                     data_type=data_type)
        return input_files

    def subset_input_files(self, time_info):
        """! Obtain a subset of input files from the c_dict ALL_FILES based on
             the time information for the current run.

              @param time_info dictionary containing time information
              @returns the path to a ascii file containing the list of files
               or None if could not find any files
        """
        fcst_files = []
        anly_files = []
        for file_dict in self.c_dict['ALL_FILES']:
            # compare time information for each input file
            # add file to list of files to use if it matches
            if not self.compare_time_info(time_info, file_dict['time_info']):
                continue

            fcst_files.extend(file_dict['fcst'])
            anly_files.extend(file_dict['anly'])

        return fcst_files, anly_files

    def compare_time_info(self, runtime, filetime):
        if not super().compare_time_info(runtime, filetime):
            return False

        # compare storm_id
        if runtime['storm_id'] == '*':
            return True

        return bool(filetime['storm_id'] == runtime['storm_id'])

    def create_ascii_storm_files_list(self, time_info, storm_id, lead_group):
        """! Creates the list of ASCII files that contain the storm id and init
             times.  The list is used to create an ASCII file which will be
             used as the option to the -obs or -fcst flag to the MET
             series_analysis tool.

             @param time_info dictionary containing time information
             @param storm_id storm ID to process
             @param lead_group dictionary where key is label and value is a
              list of forecast leads to process. If no label was defined, the
              key will match the format "NoLabel_<n>" and if no lead groups
              are defined, the dictionary should be replaced with None
        """
        time_info['storm_id'] = storm_id
        all_fcst_files = []
        all_anly_files = []
        if not lead_group:
            fcst_files, anly_files = self.subset_input_files(time_info)
            if not fcst_files or not anly_files:
                return None, None
            all_fcst_files.extend(fcst_files)
            all_anly_files.extend(anly_files)
            label = ''
            leads = None
        else:
            label = lead_group[0]
            leads = lead_group[1]
            for lead in leads:
                time_info['lead'] = lead
                fcst_files, anly_files = self.subset_input_files(time_info)
                if fcst_files and anly_files:
                    all_fcst_files.extend(fcst_files)
                    all_anly_files.extend(anly_files)

        # skip if no files were found
        if not all_fcst_files or not all_anly_files:
            return None, None

        output_dir = self.get_output_dir(time_info, storm_id, label)

        # create forecast file list
        fcst_ascii_filename = self.get_ascii_filename('FCST',
                                                      storm_id,
                                                      leads)
        self.write_list_file(fcst_ascii_filename,
                             all_fcst_files,
                             output_dir=output_dir)

        # create analysis file list
        anly_ascii_filename = self.get_ascii_filename('ANLY',
                                                      storm_id,
                                                      leads)
        self.write_list_file(anly_ascii_filename,
                             all_anly_files,
                             output_dir=output_dir)

        fcst_path = os.path.join(output_dir, fcst_ascii_filename)
        obs_path = os.path.join(output_dir, anly_ascii_filename)

        return fcst_path, obs_path

    def get_ascii_filename(self, data_type, storm_id, leads):
        if data_type == 'FCST':
            prefix = self.FCST_ASCII_FILE_PREFIX
        elif data_type == 'ANLY':
            prefix = self.ANLY_ASCII_FILE_PREFIX
        else:
            self.log_error("Invalid data type specified for "
                           f"get_ascii_filename: {data_type}")
            return None

        # of storm ID is set (not wildcard), then add it to filename
        if storm_id == '*':
            filename = ''
        else:
            filename = f"_{storm_id}"

        # add forecast leads if specified
        if leads is not None:
            lead_hours_list = [ti_get_hours_from_lead(item) for
                                 item in leads]
            # get first forecast lead, convert to hours, and add to filename
            lead_hours = min(lead_hours_list)
            lead_str = str(lead_hours).zfill(3)
            filename += f"_F{lead_str}"

            # if list of forecast leads, get min and max and add them to name
            if len(lead_hours_list) > 1:
                max_lead_hours = max(lead_hours_list)
                max_lead_str = str(max_lead_hours).zfill(3)
                filename += f"_F{max_lead_str}"

        ascii_filename = f"{prefix}{filename}"
        return ascii_filename

    def get_output_dir(self, time_info, storm_id, label):
        output_dir_template = os.path.join(self.c_dict['OUTPUT_DIR'],
                                           self.c_dict['OUTPUT_TEMPLATE'])
        output_dir_template = os.path.dirname(output_dir_template)
        if storm_id == '*':
            storm_id_out = 'all_storms'
        else:
            storm_id_out = storm_id

        # get output directory including storm ID and label
        time_info['storm_id'] = storm_id_out
        time_info['label'] = label
        output_dir = do_string_sub(output_dir_template,
                                   **time_info)
        return output_dir

    def build_and_run_series_request(self, time_info, fcst_path, obs_path):
        """! Build up the -obs, -fcst, -out necessary for running the
             series_analysis MET tool, then invoke series_analysis.

             @param time_info dictionary containing time information for
             current run
             @param storm_id storm ID to process
             @returns True if all runs succeeded, False if there was a problem
             with any of the runs
        """
        success = True

        # build the command and run series_analysis for each variable
        for var_info in self.c_dict['VAR_LIST']:
            self.infiles.append(f"-fcst {fcst_path}")
            self.infiles.append(f"-obs {obs_path}")
            self.add_field_info_to_time_info(time_info, var_info)
            self.set_environment_variables(time_info, var_info)

            self.find_and_check_output_file(time_info)

            if not self.build():
                success = False

            self.clear()

        return success

    def set_environment_variables(self, time_info, var_info):
        """! Set the env variables based on settings in the METplus config
             files.

             @param time_info dictionary containing time information
             @param var_info dictionary containing field information
        """
        self.logger.info('Setting env variables from config file...')

        # Set all the environment variables that are needed by the
        # MET config file.
        # Set up the environment variable to be used in the Series Analysis
        tmp_stat_string = str(self.c_dict['STAT_LIST'])
        tmp_stat_string = tmp_stat_string.replace("\'", "\"")
        os.environ['STAT_LIST'] = tmp_stat_string
        self.add_env_var('STAT_LIST', tmp_stat_string)
        #        self.add_env_var('STAT_LIST', self.c_dict.get('OUTPUT_STATS_CNT', ''))

        # set MODEL and REGRID_TO_GRID environment variables
        self.add_common_envs()

        # Set the NAME and LEVEL environment variables
        self.add_env_var('NAME', var_info['fcst_name'])
        self.add_env_var('LEVEL', var_info['fcst_level'])

        super().set_environment_variables(time_info)

    def get_command(self):
        cmd = self.app_path + " "

        cmd += ' '.join(self.infiles)

        cmd += f" -config {self.c_dict['CONFIG_FILE']}"

        if not self.get_output_path():
            self.logger.info("No output directory specified, because series "
                             "analysis has multiple directories")
            self.logger.info("No output filename specified, because "
                             "series analysis has multiple files")
        else:
            cmd += " -out " + os.path.join(self.get_output_path())

        return cmd

    def generate_plots(self, time_info, storm_id):
        """! Generate the plots from the series_analysis output.

             @param time_info dictionary containing time information
             @param storm_id storm ID to process
        """
        output_dir_template = os.path.join(self.c_dict['OUTPUT_DIR'],
                                           self.c_dict['OUTPUT_TEMPLATE'])

        for var_info in self.c_dict['VAR_LIST']:
            level = var_info['fcst_level']
            self.add_field_info_to_time_info(time_info, var_info)

            if storm_id == '*':
                time_info['storm_id'] = 'all_storms'
            else:
                time_info['storm_id'] = storm_id
            # get the output directory where the series_analysis output
            # was written. Plots will be written to the same directory
            plot_input = do_string_sub(output_dir_template,
                                       **time_info)

            # Get the number of forecast tile files and the name of the
            # first and last in the list to be used in the -title
            num, beg, end = (
                self.get_fcst_file_info(os.path.dirname(plot_input),
                                        storm_id)
            )
            if num is None:
                self.logger.debug(f"Skipping plot for {storm_id}")
                continue

            # Assemble the input file, output file, field string, and title
            for cur_stat in self.c_dict['STAT_LIST']:
                min, max = self.get_netcdf_min_max(plot_input,
                                                   cur_stat)

                plot_output = (f"{os.path.splitext(plot_input)[0]}_"
                               f"{cur_stat}.ps")

                time_info['num_leads'] = num
                time_info['fcst_beg'] = beg
                time_info['fcst_end'] = end
                time_info['stat'] = cur_stat
                self.plot_data_plane.c_dict['INPUT_TEMPLATE'] = plot_input
                self.plot_data_plane.c_dict['OUTPUT_TEMPLATE'] = plot_output
                self.plot_data_plane.c_dict['FIELD_NAME'] = f"series_cnt_{cur_stat}"
                self.plot_data_plane.c_dict['FIELD_LEVEL'] = level
                self.plot_data_plane.run_at_time_once(time_info)
                self.all_commands.extend(self.plot_data_plane.all_commands)
                self.plot_data_plane.all_commands.clear()

    def get_fcst_file_info(self, output_dir, storm_id):
        """! Get the number of all the gridded forecast n x m tile
            files for a given storm id and init time
            (that were created by extract_tiles). Determine the filename of the
            first and last files.  This information is used to create
            the title value to the -title opt in plot_data_plane.

            @param output_dir Directory containing ASCII list file of
             forecast files to process
            @param storm_id ID of storm to process
            @returns num, beg, end:  A tuple representing the number of
            forecast tile files, and the first and last file. If info cannot
            be parsed, return (None, None, None)
        """
        fcst_path = os.path.join(output_dir,
                                 f"{self.FCST_ASCII_FILE_PREFIX}_{storm_id}")
        # read the file but skip the first line because it contains 'file_list'
        with open(fcst_path, 'r') as file_handle:
            files_of_interest = file_handle.readlines()[1:]

        # Get a sorted list of the forecast tile files for the init
        # time of interest for all the storm ids and return the
        # forecast hour corresponding to the first and last file.
        sorted_files = sorted(files_of_interest)
        if not files_of_interest:
            self.log_error(f"No files found in file list: {fcst_path}")
            return None, None, None

        first = sorted_files[0]
        last = sorted_files[-1]

        # Extract the forecast hour from the first and last filenames.
        fcst_regex = f".*{self.c_dict['FCST_TILE_PREFIX']}" + "([0-9]{3}).*.nc"
        match_beg = re.search(fcst_regex, first)
        match_end = re.search(fcst_regex, last)

        if match_beg:
            beg = f"F{match_beg.group(1)}"
        else:
            self.log_error("Unexpected file format encountered, exiting...")
            return None, None, None

        if match_end:
            end = f"F{match_end.group(1)}"
        else:
            self.log_error("Unexpected file format encountered, exiting...")
            return None, None, None

        # Get the number of forecast tile files
        num = str(len(sorted_files))

        return num, beg, end
