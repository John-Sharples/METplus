#!/usr/bin/env python
from __future__ import print_function

import re
import os
import sys
import errno
import glob
import produtil.setup
from produtil.run import batchexe
from produtil.run import run
from command_builder import CommandBuilder
import met_util as util
import config_metplus


## @namespace SeriesByLeadWrapper
# @brief Performs any optional filtering of input tcst data then performs
# regridding via either MET regrid_data_plane or wgrib2, then builds up
# the commands to perform a series analysis by lead time by invoking the
# MET tool series_analysis. NetCDF plots are generated by invoking the MET tool
# plot_data_plane. The NetCDF plots are then converted to .png and Postscript,
# and an animated GIF representative of the entire series is generated.
#
# Call as follows:
# @code{.sh}
# SeriesByLeadWrapper.py [-c /path/to/user.template.conf]
# @endcode

# pylint:disable=too-many-instance-attributes
# all the attributes are necessary for performing tasks.
class SeriesByLeadWrapper(CommandBuilder):
    """! @brief SeriesByLeadWrapper performs series analysis of paired
         data based on lead time and generates plots for each requested
         variable and statistic, as specified in a configuration/parameter
         file.
    """

    def __init__(self, p, logger):
        super(SeriesByLeadWrapper, self).__init__(p, logger)
        self.p = p
        self.logger = logger
        if self.logger is None:
            self.logger = util.get_logger(self.p)
        # Retrieve any necessary values from the parm file(s)
        self.fhr_beg = p.getint('config', 'FHR_BEG')
        self.fhr_end = p.getint('config', 'FHR_END')
        self.fhr_inc = p.getstr('config', 'FHR_INC')
        self.fhr_group_beg_str = util.getlist(p.getstr('config',
                                                       'FHR_GROUP_BEG'))
        self.fhr_group_end_str = util.getlist(p.getstr('config',
                                                       'FHR_GROUP_END'))
        self.fhr_group_beg = [int(beg) for beg in self.fhr_group_beg_str]
        self.fhr_group_end = [map(int, end) for end in self.fhr_group_end_str]
        self.fhr_group_labels = util.getlist(p.getstr('config',
                                                      'FHR_GROUP_LABELS'))
        self.var_list = util.getlist(p.getstr('config', 'VAR_LIST'))
        self.stat_list = util.getlist(p.getstr('config', 'STAT_LIST'))
        self.plot_data_plane_exe = os.path.join(
            self.p.getdir('MET_INSTALL_DIR'),
            'bin/plot_data_plane')
        self.convert_exe = p.getexe('CONVERT_EXE')
        self.ncap2_exe = p.getexe('NCAP2_EXE')
        self.ncdump_exe = p.getexe('NCDUMP_EXE')
        self.rm_exe = p.getexe("RM_EXE")
        met_install_dir = self.p.getdir('MET_INSTALL_DIR')
        self.series_analysis_exe = os.path.join(met_install_dir,
                                                'bin/series_analysis')
        self.extract_tiles_dir = p.getdir('EXTRACT_OUT_DIR')
        self.series_lead_filtered_out_dir = \
            p.getdir('SERIES_LEAD_FILTERED_OUT_DIR')
        self.series_lead_out_dir = p.getdir('SERIES_LEAD_OUT_DIR')
        self.tmp_dir = p.getdir('TMP_DIR')
        self.background_map = p.getbool('config', 'BACKGROUND_MAP')
        self.regrid_with_met_tool = \
            p.getbool('config', 'REGRID_USING_MET_TOOL')
        self.series_filter_opts = \
            p.getstr('config', 'SERIES_ANALYSIS_FILTER_OPTS')
        self.series_filter_opts.strip()
        self.fcst_ascii_regex = \
            p.getstr('regex_pattern', 'FCST_ASCII_REGEX_LEAD')
        self.anly_ascii_regex = \
            p.getstr('regex_pattern', 'ANLY_ASCII_REGEX_LEAD')
        self.series_anly_configuration_file = \
            p.getstr('config', 'SERIES_ANALYSIS_BY_LEAD_CONFIG_FILE')
        self.var_list = util.getlist(p.getstr('config', 'VAR_LIST'))
        self.regrid_with_met_tool = p.getbool('config',
                                              'REGRID_USING_MET_TOOL')
        if self.regrid_with_met_tool:
            # Re-gridding via MET Tool regrid_data_plane.
            self.fcst_tile_regex = \
                self.p.getstr('regex_pattern', 'FCST_NC_TILE_REGEX')
            self.anly_tile_regex = \
                self.p.getstr('regex_pattern', 'ANLY_NC_TILE_REGEX')
        else:
            # Re-gridding via wgrib2 tool.
            self.fcst_tile_regex = self.p.getstr('regex_pattern',
                                                 'FCST_TILE_REGEX')
            self.anly_tile_regex = self.p.getstr('regex_pattern',
                                                 'ANLY_TILE_REGEX')

    def run_all_times(self):
        """! Perform a series analysis of extra tropical cyclone
             paired data based on lead time (forecast hour)
             This requires invoking the MET run_series_analysis binary,
             followed by generating graphics that are recognized by
             the MET viewer using the plot_data_plane and converting to
             postscript.
             A pre-requisite is the presence of the filter file and storm files
             (set to nxm degree tiles as indicated in the param/config file)
             the specified init and lead times.

             Create the following command to satisfy MET series_analysis:
             series_analysis -fcst <FILTERED_OUT_DIR>/FCST_FILES_F<CUR_FHR>
                         -obs <FILTERED_OUT_DIR>/ANLY_FILES_F<CUR_FHR>
                         -out <OUT_DIR>/series_F<CURR_FHR_<NAME>_<LEVEL>.nc
                         -config SeriesAnalysisConfig_by_lead
            Args:

            Returns:
                None:   Invokes MET series_analysis and any other MET
                        tool to perform series analysis.  Then plots
                        are generated for the variables and statistics (as
                        indicated in the param/config file) corresponding to
                        each forecast lead time.
    """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Flag used to determine whether to use the forecast hour range and
        # increment, or the specified list of forecast hours in creating the
        # Series-analysis command.
        # Support for GitHub Issue #3
        do_fhr_by_range = True

        # pylint:disable=len-as-condition
        # modifying to if self.fhr_group_beg never sets do_fhr_by_range=False
        # when the fhr_group_beg is empty.  In this case, pylint warning
        # should be ignored.
        # GitHub Issue #3 support grouping forecast hours.
        if len(self.fhr_group_beg) > 0:
            do_fhr_by_range = False

        # Set up the environment variable to be used in the Series Analysis
        #   Config file (SERIES_ANALYSIS_BY_LEAD_CONFIG_FILE)
        # Used to set cnt value in output_stats in
        # "SERIES_ANALYSIS_BY_LEAD_CONFIG_FILE"
        # Need to do some pre-processing so that Python will use " and not '
        #  because currently MET doesn't support single-quotes
        tmp_stat_string = str(self.stat_list)
        tmp_stat_string = tmp_stat_string.replace("\'", "\"")

        # For example, we want tmp_stat_string to look like
        #   '["TOTAL","FBAR"]', NOT "['TOTAL','FBAR']"
        os.environ['STAT_LIST'] = tmp_stat_string

        self.logger.info("Begin series analysis by lead...")

        # Initialize the tile_dir to the extract tiles output directory.
        # And retrieve a list of init times based on the data available in
        # the extract tiles directory.
        tile_dir = self.extract_tiles_dir
        init_times = util.get_updated_init_times(tile_dir, self.logger)

        # Check for the existence of the storm track tiles and raise
        # an error if these are missing.
        try:
            util.check_for_tiles(tile_dir, self.fcst_tile_regex,
                                 self.anly_tile_regex, self.logger)
        except OSError:
            msg = ("ERROR|[ " + cur_filename + ":" +
                   cur_function + "]| Missing 30x30 tile files." +
                   "  Extract tiles needs to be run")
            self.logger.error(msg)

        # Apply optional filtering via tc_stat, as indicated in the
        # parameter/config file.
        tile_dir = self.filter_with_tc_stat(tile_dir, init_times)

        # GitHub issue #30
        # gracefully handle user's intent to process only one forecast hour:
        # i.e.: FCST_INIT=FCST_END and FCST_INCR=0
        # If the user sets FCST_INCR=0 but has FCST_INIT != FCST_END, then
        # log an error and exit.
        fhr_diff = self.fhr_end - self.fhr_beg
        if fhr_diff == 0 and self.fhr_inc == 0:
            self.fhr_inc = 1
        elif self.fhr_inc == 0:
            self.logger.error('ERROR: ' + os.strerror(errno.EINVAL) +
                              ' fcst range indicated with increment '
                              'of 0 hrs, please check the configuration' +
                              'file.  Exiting...')
            sys.exit(errno.EINVAL)
        # GitHub Issue #3 support grouping by forecast hours:
        if do_fhr_by_range:
            # entire range of forecast hours
            start = int(self.fhr_beg)
            end = int(self.fhr_end) + 1
            step = int(self.fhr_inc)
        else:
            # Grouping by forecast hours
            start_list_len = len(self.fhr_group_beg)
            end_list_len = len(self.fhr_group_end)
            label_list_len = len(self.fhr_group_labels)
            step = self.fhr_inc

            # Check that the number of start and end fhrs are the same, ie
            # each value in FHR_GROUP_BEG has a corresponding ending fhr in
            # FHR_GROUP_END.
            if start_list_len != end_list_len or \
                            start_list_len != label_list_len or \
                            end_list_len != label_list_len:
                self.logger.error("ERROR:|" + os.strerror(errno.EINVAL) +
                                  " The number of forecast hour begin " +
                                  "and end times, and the number of " +
                                  "corresponding labels must ALL be " +
                                  "identical.  Exiting...")
                sys.exit(errno.EINVAL)

        # Build up the command for MET series-analysis using either a list of
        # specified forecast hours, or a range of forecast hours with
        # increment.
        if do_fhr_by_range:
            self.logger.debug("performing series analysis on entire range"
                              " of fhrs...")
            self.perform_series_for_all_fhrs(tile_dir, start, end, step)
        else:
            # Perform series analysis on groupings of forecast hours
            # specified in the METplus config file.
            self.logger.debug("performing series analysis on groupings of"
                              " forecast hours...")
            self.perform_series_for_fhr_groups(tile_dir)

        # Generate plots in NetCDF, png, and Postscript.
        self.generate_plots(do_fhr_by_range)

        # Create animated gif
        self.create_animated_gifs(do_fhr_by_range)

        self.logger.info("Finished with series analysis by lead")

    def filter_with_tc_stat(self, tile_dir, init_times):
        """! Perform optional filtering using MET tc_stat

            Args:
              @param tile_dir: The directory where the input data resides.
              @param init_times: A list of init times under which series
              filters will be applied.
            Returns:
              filter_tile_dir:  A directory of the resulting files from applying
                         the filter criteria (as specified in the param/
                         config file).
        """
        if self.series_filter_opts:
            util.mkdir_p(self.series_lead_filtered_out_dir)
            util.apply_series_filters(tile_dir, init_times,
                                      self.series_lead_filtered_out_dir,
                                      self.series_filter_opts,
                                      self.tmp_dir,
                                      self.logger,
                                      self.p)

            # Remove any empty files and directories to avoid
            # errors or performance degradation when performing
            # series analysis.
            util.prune_empty(self.series_lead_filtered_out_dir, self.logger)

            # Get the list of all the files that were created as a result
            # of applying the filter options.  Save this information, it
            # will be useful for troubleshooting and validating the correctness
            # of filtering.

            # First, make sure that the series_lead_filtered_out directory
            # isn't empty.  If so, then no files fall within the filter
            # criteria.
            if os.listdir(self.series_lead_filtered_out_dir):
                # Filtering produces results, assign the tile_dir to
                # the filter output directory, series_lead_filtered_out_dir.
                filtered_files_list = util.get_files(tile_dir, ".*.",
                                                     self.logger)

                # Create the tmp_fcst and tmp_anly ASCII files containing the
                # list of files that meet the filter criteria.
                util.create_filter_tmp_files(filtered_files_list,
                                             self.series_lead_filtered_out_dir)
                filter_tile_dir = self.series_lead_filtered_out_dir
            else:
                # No data meet filter criteria, use data from extract
                #  tiles directory.
                msg = ("WARN| After applying filter options, no data meet"
                       " filter criteria. Continue using all available data "
                       "in extract tiles directory.")
                self.logger.debug(msg)
                filter_tile_dir = self.extract_tiles_dir

        else:
            # No additional filtering was requested.  The extract tiles
            # directory is the
            # source of input tile data.
            filter_tile_dir = self.extract_tiles_dir

        return filter_tile_dir

    def perform_series_for_fhr_groups(self, tile_dir):
        """! Series analysis for groups based on forecast hours

              Args:
                  @param tile_dir:  The location where input data resides.
              Returns:       None

        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # Used for logging.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Since we checked that the number of fhr start, end, and labels
        # were requested, we can use the size of one of these lists to
        # determine how many forecast hour groupings exist.
        num_of_groups = len(self.fhr_group_beg)

        fcst_tiles_list = []
        anly_tiles_list = []

        self.logger.debug('DEBUG|' + cur_filename + '|' + cur_function +
                          ' Performing series analysis on forecast hour'
                          ' groupings.')

        # Create the output directory where the series analysis results
        # will be saved.
        #
        # Build up the arguments used to run MET series_analysis:
        # 1) Create groupings of the forecast gridded tiles and
        #    analysis gridded tiles so they can be saved in ASCII files to be
        #    used for the -fcst and -obs arguments.
        # 2) Build up the -out argument
        # 3) Create the full command for running MET series_analysis by
        #    combining the -fcst -obs, -out and other arguments.

        util.mkdir_p(self.series_lead_out_dir)
        for group in range(num_of_groups):
            cur_label = self.fhr_group_labels[group]
            cur_beg_str = self.fhr_group_beg_str[group].zfill(3)
            cur_end_str = self.fhr_group_end_str[group].zfill(3)
            cur_beg = int(cur_beg_str)
            cur_end = int(cur_end_str)

            out_dir_parts = [self.series_lead_out_dir, '/', cur_label]
            out_dir = ''.join(out_dir_parts)
            util.mkdir_p(out_dir)
            msg = ('DEBUG|' + cur_filename + '|' + cur_function + ' | '
                   'Evaluating forecast hours: ' + cur_beg_str + ' to ' +
                   cur_end_str)
            self.logger.debug(msg)

            # Loop over each forecast hour within each group
            # for example, if the FHR_GROUP_BEG of the first group is 24 hours
            # and the FHR_GROUP_END of the first group is 42 hours and the
            # forecast hours are incremented by 6 hours, then we will iterate
            # over the 24, 30, 36, and 42 hour forecast times.
            inc_hr = int(self.fhr_inc)
            for cur_fhr in range(cur_beg, cur_end + inc_hr, inc_hr):
                cur_fcst_tiles_list = self.get_anly_or_fcst_files(
                    tile_dir, "FCST", self.fcst_tile_regex,
                    cur_fhr)

                cur_anly_tiles_list = self.get_anly_or_fcst_files(
                    tile_dir, "ANLY", self.anly_tile_regex, cur_fhr)

                # Location of "grouped" FCST_FILES_Fhhh and ANLY_FILES_Fhhh
                ascii_fcst_file_parts = [out_dir, '/FCST_FILES_F',
                                         cur_beg_str + '_to_F' + cur_end_str]
                ascii_anly_file_parts = [out_dir, '/ANLY_FILES_F',
                                         cur_beg_str + '_to_F' + cur_end_str]
                ascii_fcst_file = ''.join(ascii_fcst_file_parts)
                ascii_anly_file = ''.join(ascii_anly_file_parts)

                # Iterate over each forecast hour beg, end pair in this
                # grouping
                for cur_fcst in cur_fcst_tiles_list:
                    fcst_tiles_list.append(cur_fcst)

                for cur_anly in cur_anly_tiles_list:
                    anly_tiles_list.append(cur_anly)

                # Create the FCST and ANLY ASCII files that are the args
                # to the -fcst and -obs portion of the series_analysis
                # command.

                # For FCST
                try:
                    if not fcst_tiles_list:
                        msg = ("INFO|[" + cur_filename + ":" +
                               cur_function +
                               " No fcst_tiles for fhr group: " + cur_beg_str +
                               " to " + cur_end_str +
                               " Don't create FCST_F<fhr> ASCII file")
                        self.logger.debug(msg)
                    else:
                        with open(ascii_fcst_file, 'a') as file_handle:
                            for fcst_tiles in fcst_tiles_list:
                                file_handle.write(fcst_tiles)
                                file_handle.write('\n')
                except IOError as io_error:
                    msg = ("ERROR: Could not create requested" +
                           " ASCII file: " + ascii_fcst_file + " | ")
                    self.logger.error(msg + io_error)

                # For ANLY
                try:
                    if not anly_tiles_list:
                        msg = ("INFO|[" + cur_filename + ":" +
                               cur_function +
                               " No anly_tiles for fhr group: " +
                               str(cur_beg) + " to " + str(cur_end) +
                               " Don't create ANLY_F<fhr> ASCII file")
                        self.logger.debug(msg)
                    else:
                        with open(ascii_anly_file, 'a') as file_handle:
                            for anly_tiles in anly_tiles_list:
                                file_handle.write(anly_tiles)
                                file_handle.write('\n')

                except IOError as io_error:
                    msg = ("ERROR: Could not create requested" +
                           " ASCII file: " + ascii_anly_file + " | ")
                    self.logger.error(msg + io_error)

                # Remove any empty directories that were created when no
                # files were written.
                util.prune_empty(out_dir, self.logger)

                # Create the -fcst and -obs portion of the series_analysis
                # command.
                fcst_param_parts = ['-fcst ', ascii_fcst_file]
                fcst_param = ''.join(fcst_param_parts)
                obs_param_parts = ['-obs ', ascii_anly_file]
                obs_param = ''.join(obs_param_parts)
                self.logger.debug('fcst param: ' + fcst_param)
                self.logger.debug('obs param: ' + obs_param)

                # Create the -out portion of the series_analysis command.
                for cur_var in self.var_list:
                    # Get the name and level to create the -out param
                    # and set the NAME and LEVEL environment variables that
                    # are needed by the MET series analysis binary.
                    match = re.match(r'(.*)/(.*)', cur_var)
                    name = match.group(1)
                    level = match.group(2)
                    os.environ['NAME'] = name
                    os.environ['LEVEL'] = level

                    # Set the NAME environment to <name>_<level> format if
                    # regridding method is to be done with the MET tool
                    # regrid_data_plane (which is  indicated in the
                    # config/param file).
                    if self.regrid_with_met_tool:
                        os.environ['NAME'] = name + '_' + level
                    out_param_parts = ['-out ', out_dir, '/series_F',
                                       cur_beg_str, '_to_F', cur_end_str,
                                       '_', name, '_', level, '.nc']
                    out_param = ''.join(out_param_parts)

                    # Create the full series analysis command.
                    config_param_parts = ['-config ',
                                          self.series_anly_configuration_file]
                    config_param = ''.join(config_param_parts)
                    series_analysis_cmd_parts = [self.series_analysis_exe, ' ',
                                                 ' -v 4 ', fcst_param, ' ',
                                                 obs_param, ' ', config_param,
                                                 ' ', out_param]
                    series_analysis_cmd = ''.join(series_analysis_cmd_parts)
                    msg = ("INFO:[ " + cur_filename + ":" +
                           cur_function + "]|series analysis command: " +
                           series_analysis_cmd)
                    self.logger.debug(msg)
                    series_analysis_cmd = batchexe('sh')[
                        '-c', series_analysis_cmd].err2out()
                    run(series_analysis_cmd)

                    # Clean up any empty files and directories that still
                    # persist.
                    util.prune_empty(self.series_lead_out_dir, self.logger)

    def perform_series_for_all_fhrs(self, tile_dir, start, end, step):
        """! Performs a series analysis by lead time, based on a range and
             increment of forecast hours. Invokes the MET tool Series-analysis

             Args:
                   @param tile_dir:  The location of the input data (output
                                     from running ExtractTiles.py)
                   @param start:     The first forecast hour
                   @param end:       The last forecast hour
                   @param step:      The time increment/step size between
                                     forecast hours

             Returns:          None
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # Used for logging.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        for fhr in range(start, end, step):
            cur_fhr = str(fhr).zfill(3)
            msg = ('INFO|[' + cur_filename + ':' + cur_function +
                   ']| Evaluating forecast hour ' + cur_fhr)
            self.logger.debug(msg)

            # Create the output directory where the netCDF series files
            # will be saved.
            util.mkdir_p(self.series_lead_out_dir)
            out_dir_parts = [self.series_lead_out_dir, '/', 'series_F',
                             cur_fhr]
            out_dir = ''.join(out_dir_parts)
            util.mkdir_p(out_dir)

            # Gather all the forecast gridded tile files
            # so they can be saved in ASCII files.
            fcst_tiles_list = self.get_anly_or_fcst_files(tile_dir, "FCST",
                                                          self.fcst_tile_regex,
                                                          cur_fhr)
            fcst_tiles = self.retrieve_fhr_tiles(fcst_tiles_list,
                                                 self.fcst_tile_regex)

            # Location of FCST_FILES_Fhhh
            ascii_fcst_file_parts = [out_dir, '/FCST_FILES_F', cur_fhr]
            ascii_fcst_file = ''.join(ascii_fcst_file_parts)

            # Now create the ASCII files needed for the -fcst and -obs
            try:
                if not fcst_tiles:
                    msg = ("INFO|[" + cur_filename + ":" +
                           cur_function + " No fcst_tiles for fhr: " +
                           cur_fhr + " Don't create FCST_F<fhr> ASCII file")
                    self.logger.debug(msg)
                    continue
                else:
                    with open(ascii_fcst_file, 'a') as file_handle:
                        file_handle.write(fcst_tiles)

            except IOError as io_error:
                msg = ("ERROR: Could not create requested" +
                       " ASCII file: " + ascii_fcst_file + " | ")
                self.logger.error(msg + io_error)

            # Gather all the anly gridded tile files
            # so they can be saved in ASCII files.
            anly_tiles_list = self.get_anly_or_fcst_files(tile_dir, "ANLY",
                                                          self.anly_tile_regex,
                                                          cur_fhr)
            anly_tiles = self.retrieve_fhr_tiles(anly_tiles_list,
                                                 self.anly_tile_regex)

            # Location of ANLY_FILES_Fhhh files
            # filtering.
            ascii_anly_file_parts = [out_dir, '/ANLY_FILES_F', cur_fhr]
            ascii_anly_file = ''.join(ascii_anly_file_parts)

            try:
                # Only write to the ascii_anly_file if
                # the anly_tiles string isn't empty.
                if not anly_tiles:
                    msg = ("INFO|[" + cur_filename + ":" +
                           cur_function + "No anly_tiles for fhr: " +
                           cur_fhr + " Don't create ANLY_F<fhr> ASCII file")
                    self.logger.debug(msg)
                    continue
                else:
                    with open(ascii_anly_file, 'a') as file_handle:
                        file_handle.write(anly_tiles)

            except IOError:
                self.logger.error("ERROR: Could not create requested " +
                                  "ASCII file: " + ascii_anly_file)

            # Remove any empty directories that result from
            # when no files are written.
            util.prune_empty(out_dir, self.logger)
            self.logger.debug("DEBUG: finished pruning empty files")

            # -fcst and -obs params
            fcst_param_parts = ['-fcst ', ascii_fcst_file]
            fcst_param = ''.join(fcst_param_parts)
            obs_param_parts = ['-obs ', ascii_anly_file]
            obs_param = ''.join(obs_param_parts)
            self.logger.debug('fcst param: ' + fcst_param)
            self.logger.debug('obs param: ' + obs_param)

            # Create the -out param and invoke the MET series
            # analysis binary
            for cur_var in self.var_list:
                # Get the name and level to create the -out param
                # and set the NAME and LEVEL environment variables that
                # are needed by the MET series analysis binary.
                match = re.match(r'(.*)/(.*)', cur_var)
                name = match.group(1)
                level = match.group(2)
                os.environ['NAME'] = name
                os.environ['LEVEL'] = level

                # Set NAME to name_level if regridding with regrid data plane
                if self.regrid_with_met_tool:
                    os.environ['NAME'] = name + '_' + level
                    out_param_parts = ['-out ', out_dir, '/series_F', cur_fhr,
                                       '_', name, '_', level, '.nc']
                    out_param = ''.join(out_param_parts)

                # Create the full series analysis command.
                config_param_parts = ['-config ',
                                      self.series_anly_configuration_file]
                config_param = ''.join(config_param_parts)
                series_analysis_cmd_parts = [self.series_analysis_exe, ' ',
                                             ' -v 4 ',
                                             fcst_param, ' ', obs_param,
                                             ' ', config_param, ' ',
                                             out_param]
                series_analysis_cmd = ''.join(series_analysis_cmd_parts)
                msg = ("INFO:[ " + cur_filename + ":" +
                       cur_function + "]|series analysis command: " +
                       series_analysis_cmd)
                self.logger.debug(msg)
                series_analysis_cmd = \
                    batchexe('sh')['-c', series_analysis_cmd].err2out()
                run(series_analysis_cmd)

                # Make sure there aren't any emtpy
                # files or directories that still persist.
        util.prune_empty(self.series_lead_out_dir, self.logger)

    def get_nseries(self, do_fhr_by_range, nc_var_file):
        """! Determine the number of series for this lead time and
           its associated variable via calculating the max series_cnt_TOTAL
           value, maximum.

           Args:
              @param do_fhr_by_range:  Boolean value indicating whether series
                                analysis was performed on a range of forecast
                                hours (True) or on a "bucket" of forecast hours
                                (False).
              @param nc_var_file:  The netCDF file for a particular variable.

           Returns:
                 maximum (float): The maximum value of series_cnt_TOTAL of all
                                  the netCDF files for the variable cur_var.
                 None:          If no max value is found.
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.

        # Useful for logging
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Determine the series_F<fhr> subdirectory where this netCDF file
        # resides.
        if do_fhr_by_range:
            match = re.match(r'(.*/series_F[0-9]{3})/series_F[0-9]{3}.*nc',
                             nc_var_file)
        else:
            match = re.match(r'(.*/.*/)series_F[0-9]{3}_to_F[0-9]{3}.*nc',
                             nc_var_file)

        if match:
            base_nc_dir = match.group(1)
        else:
            msg = ("ERROR|[" + cur_filename + ":" +
                   cur_function + "]| " +
                   "Cannot determine base directory path for " +
                   "netCDF files... exiting")
            self.logger.error(msg)
            sys.exit(1)

        # Use NCO utility ncap2 to find the max for the variable and
        # series_cnt_TOTAL pair.
        nseries_nc_path = os.path.join(base_nc_dir, 'nseries.nc')

        nco_nseries_cmd_parts = [self.ncap2_exe, ' -v -s ', '"',
                                 'max=max(series_cnt_TOTAL)', '" ',
                                 nc_var_file, ' ', nseries_nc_path]
        nco_nseries_cmd = ''.join(nco_nseries_cmd_parts)
        nco_nseries_cmd = batchexe('sh')['-c', nco_nseries_cmd].err2out()
        run(nco_nseries_cmd)

        # Create an ASCII file with the max value, which can be parsed.
        nseries_txt_path = os.path.join(base_nc_dir, 'nseries.txt')

        ncdump_max_cmd_parts = [self.ncdump_exe, ' ', nseries_nc_path,
                                '> ', nseries_txt_path]
        ncdump_max_cmd = ''.join(ncdump_max_cmd_parts)
        ncdump_max_cmd = batchexe('sh')['-c', ncdump_max_cmd].err2out()
        run(ncdump_max_cmd)

        # Look for the max value for this netCDF file.
        try:
            with open(nseries_txt_path, 'r') as fmax:
                for line in fmax:
                    max_match = re.match(r'\s*max\s*=\s([-+]?\d*\.*\d*)', line)
                    if max_match:
                        maximum = max_match.group(1)

                        # Clean up any intermediate .nc and .txt files
                        nseries_list = [self.rm_exe + ' -rf', ' ', base_nc_dir,
                                        '/nseries.txt']
                        nseries_cmd = ''.join(nseries_list)
                        os.system(nseries_cmd)

                        return maximum
                    else:
                        # Remove the nseries.nc file, it is no longer needed
                        if os.path.isfile(nseries_nc_path):
                            print("REMOVING OLD nc path file")
                            os.remove(nseries_nc_path)

        except IOError:
            msg = ("ERROR|[" + cur_filename + ":" +
                   cur_function + "]| cannot open the max text file")
            self.logger.error(msg)

    def get_netcdf_min_max(self, do_fhr_by_range, nc_var_files, cur_stat):
        """! Determine the min and max for all lead times for each
           statistic and variable pairing.

           Args:
               @param do_fhr_by_range:  Boolean value indicating whether series
                                     analysis was performed on a range of
                                     forecast hours (True) or on a grouping
                                     of forecast hours (False).
               @param nc_var_files:  A list of the netCDF files generated
                                     by the MET series analysis tool that
                                     correspond to the variable of interest.
               @param cur_stat:      The current statistic of interest: ie.
                                     RMSE, MAE, ODEV, FDEV, ME, or TOTAL.

           Returns:
               tuple (vmin, vmax)
                   vmin:  The minimum
                   vmax:  The maximum

        """
        max_temporary_files = []
        min_temporary_files = []

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Initialize the threshold values for min and max.
        vmin = 999999.
        vmax = -999999.

        for cur_nc in nc_var_files:
            # Determine the series_F<fhr> subdirectory where this
            # netCDF file resides.
            if do_fhr_by_range:
                match = re.match(r'(.*/series_F[0-9]{3})/series_F[0-9]{3}.*nc',
                                 cur_nc)
            else:
                match = re.match(r'(.*/.*)/'
                                 r'series_F[0-9]{3}_to_F[0-9]{3}.*nc', cur_nc)
            if match:
                base_nc_dir = match.group(1)
                self.logger.debug("base nc dir: " + base_nc_dir)
            else:
                msg = ("ERROR|[" + cur_filename + ":" + cur_function +
                       "]| Cannot determine base directory path " +
                       "for netCDF files. Exiting...")
                self.logger.error(msg)
                sys.exit(1)

            # Create file paths for temporary files for min value...
            min_nc_path = os.path.join(base_nc_dir, 'min.nc')
            min_txt_path = os.path.join(base_nc_dir, 'min.txt')
            min_temporary_files.append(min_nc_path)
            min_temporary_files.append(min_txt_path)

            # Clean up any temporary min files that might have been left over
            #  from a previous run.
            util.cleanup_temporary_files(min_temporary_files)

            # Use NCO ncap2 to get the min for the current stat-var pairing.
            nco_min_cmd_parts = [self.ncap2_exe, ' -v -s ', '"',
                                 'min=min(series_cnt_', cur_stat, ')',
                                 '" ', cur_nc, ' ', min_nc_path]
            nco_min_cmd = ''.join(nco_min_cmd_parts)
            self.logger.debug('nco_min_cmd: ' + nco_min_cmd)
            nco_min_cmd = batchexe('sh')['-c', nco_min_cmd].err2out()
            run(nco_min_cmd)

            # now set up file paths for the max value...
            max_nc_path = os.path.join(base_nc_dir, 'max.nc')
            max_txt_path = os.path.join(base_nc_dir, 'max.txt')
            max_temporary_files.append(max_nc_path)
            max_temporary_files.append(max_txt_path)

            # First, remove pre-existing max.txt and max.nc file from any
            #  previous run.
            util.cleanup_temporary_files(max_temporary_files)

            # Using NCO ncap2 to perform arithmetic processing to retrieve
            #  the max from each
            # netCDF file's stat-var pairing.
            nco_max_cmd_parts = [self.ncap2_exe, ' -v -s ', '"',
                                 'max=max(series_cnt_', cur_stat, ')',
                                 '" ', cur_nc, ' ', max_nc_path]
            nco_max_cmd = ''.join(nco_max_cmd_parts)
            self.logger.debug('nco_max_cmd: ' + nco_max_cmd)
            nco_max_cmd = batchexe('sh')['-c', nco_max_cmd].err2out()
            run(nco_max_cmd)

            # Create ASCII files with the min and max values, using the
            # NCO utility ncdump.
            # These files can be parsed to determine the vmin and vmax.
            ncdump_min_cmd_parts = [self.ncdump_exe, ' ', base_nc_dir,
                                    '/min.nc > ', min_txt_path]
            ncdump_min_cmd = ''.join(ncdump_min_cmd_parts)
            ncdump_min_cmd = batchexe('sh')['-c', ncdump_min_cmd].err2out()
            run(ncdump_min_cmd)

            ncdump_max_cmd_parts = [self.ncdump_exe, ' ', base_nc_dir,
                                    '/max.nc > ', max_txt_path]
            ncdump_max_cmd = ''.join(ncdump_max_cmd_parts)
            ncdump_max_cmd = batchexe('sh')['-c', ncdump_max_cmd].err2out()
            run(ncdump_max_cmd)

            # Search for 'min' in the min.txt file.
            try:
                with open(min_txt_path, 'r') as fmin:
                    for line in fmin:
                        min_match = re.match(r'\s*min\s*=\s([-+]?\d*\.*\d*)',
                                             line)
                        if min_match:
                            cur_min = float(min_match.group(1))
                            if cur_min < vmin:
                                vmin = cur_min
            except IOError:
                msg = ("ERROR|[" + cur_filename + ":" + cur_function +
                       "]| cannot open the min text file")
                self.logger.error(msg)

            # Search for 'max' in the max.txt file.
            try:
                with open(max_txt_path, 'r') as fmax:
                    for line in fmax:
                        max_match = re.match(r'\s*max\s*=\s([-+]?\d*\.*\d*)',
                                             line)
                        if max_match:
                            cur_max = float(max_match.group(1))
                            if cur_max > vmax:
                                vmax = cur_max
            except IOError:
                msg = ("ERROR|[" + cur_filename + ":" + cur_function +
                       "]| cannot open the max text file")
                self.logger.error(msg)

            # Clean up min.nc, min.txt, max.nc and max.txt temporary files.
            util.cleanup_temporary_files(min_temporary_files)
            util.cleanup_temporary_files(max_temporary_files)

        return vmin, vmax

    @staticmethod
    def get_var_ncfiles(do_fhr_by_range, cur_var, nc_list):
        """! Retrieve only the netCDF files corresponding to this statistic
            and variable pairing.

            Args:
                @param do_fhr_by_range: The boolean value indicating whether
                                     series analysis was performed on a range
                                     of forecast hours (True), or on a group of
                                     forecast hours (False)

                @param cur_var:  The variable of interest.
                @param nc_list:  The list of all netCDF files that were
                                 generated by the MET utility
                                 series_analysis.

            Returns:
                var_ncfiles: A list of netCDF files that
                                  correspond to this variable.
        """

        # Create the regex to retrieve the variable name.
        # The variable is contained in the netCDF file name.
        var_ncfiles = []
        if do_fhr_by_range:
            var_regex_parts = [".*series_F[0-9]{3}_", cur_var,
                               "_[0-9a-zA-Z]+.*nc"]
        else:
            var_regex_parts = [".*series_F[0-9]{3}_to_F[0-9]{3}_", cur_var,
                               "_[0-9a-zA-Z]+.*nc"]

        var_regex = ''.join(var_regex_parts)
        for cur_nc in nc_list:
            # Determine the variable from the filename
            match = re.match(var_regex, cur_nc)
            if match:
                var_ncfiles.append(cur_nc)

        return var_ncfiles

    def retrieve_nc_files(self, do_fhr_by_range):
        """! Retrieve all the netCDF files created by MET series_analysis.

        Args:
            @param do_fhr_by_range:  Boolean value, True if series analysis was
                                  performed on range.  False otherwise.
        Returns:
            nc_list:      A list of the netCDF files (full path) created
                          when the MET series analysis binary was invoked.
        """

        nc_list = []

        if do_fhr_by_range:
            filename_regex = "series_F[0-9]{3}.*nc"
        else:
            filename_regex = "series_F[0-9]{3}_to_F[0-9]{3}_*.*nc"
            # filename_regex = "series_F[0-9]{3}.*nc"

        # Get a list of all the series_F* directories
        # Use the met_utils function get_dirs to get only
        # the directories, as we are also generating
        # ASCII tmp_fcst and tmp_anly files in the
        # same directory, which can cause problems if included in
        # the series_dir_list.
        series_dir_list = util.get_dirs(self.series_lead_out_dir)

        # Iterate through each of these series subdirectories
        # and create a list of all the netCDF files (full file path).
        for series_dir in series_dir_list:
            full_path = os.path.join(self.series_lead_out_dir, series_dir)

            # Get a list of all the netCDF files for this subdirectory.
            nc_files_list = [f for f in os.listdir(full_path) if
                             os.path.isfile(os.path.join(full_path, f))]
            for cur_nc in nc_files_list:
                print("cur_nc: {}".format(cur_nc))
                match = re.match(filename_regex, cur_nc)
                if match:
                    nc_file = os.path.join(full_path, cur_nc)
                    nc_list.append(nc_file)

        if not nc_list:
            self.logger.warn("WARNING: empty nc_list returned")

        return nc_list

    def retrieve_fhr_tiles(self, tile_list, type_regex):
        """! Retrieves only the gridded tile files that
            correspond to the type.

            Args:
              @param tile_list:  List of tiles (full filepath).
              @param type_regex: The regex that corresponds to the tile
                        filename for this type

            Returns:
            fhr_tiles (string):  A string of gridded tile names
                                 separated by newlines
        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        fhr_tiles = ''
        for cur_tile in tile_list:
            match = re.match(type_regex, cur_tile)
            if not match:
                msg = ("ERROR|[" + cur_filename + ":" +
                       cur_function +
                       "]| No matching storm id found, exiting...")
                self.logger.error(msg)
                return ''

            fhr_tiles += cur_tile
            fhr_tiles += '\n'

        return fhr_tiles

    def find_matching_tile(self, fcst_file, anly_tiles):
        """! Find the corresponding ANLY 30x30 tile file to the
            fcst tile file.
            Args:
              @param fcst_file :  The fcst file (full path) that
                                  is used to derive the corresponding
                                  analysis file name.
              @param anly_tiles : The list of all available 30x30 analysis
                                  tiles.

            Returns:
              anly_from_fcst (string): The name of the analysis tile file
                                       that corresponds to the same lead
                                       time as the input fcst tile.
        """

        # Derive the ANLY file name from the FCST file.
        anly_from_fcst = re.sub(r'FCST', 'ANLY', fcst_file)

        if anly_from_fcst in anly_tiles:
            return anly_from_fcst
        else:
            self.logger.debug("No corresponding analysis file found: " +
                              anly_from_fcst)
            return None

    @staticmethod
    def get_anly_or_fcst_files(filedir, file_type, filename_regex,
                               cur_fhr):
        """! Get all the ANLY or FCST files by walking
            through the directories starting at filedir.

            Args:
              @param filedir:  The topmost directory from which the
                               search begins.
              @param file_type:  FCST or ANLY
              @param filename_regex:  The regular expression that
                                      defines the naming format
                                      of the files of interest.

              @param cur_fhr:  The current forecast hour for which we need to
                               find the corresponding file

            Returns:
                file_paths (string): a list of filenames (with full filepath)
        """

        file_paths = []

        # Convert cur_fhr to a string that has zero padding/filling
        cur_fhr_str = (str(cur_fhr)).zfill(3)

        # pylint:disable=unused-variable
        # os.walk returns tuple, not all returned variables are used.

        # Walk the tree
        for root, directories, files in os.walk(filedir):
            for filename in files:
                # add it to the list only if it is a match
                # to the specified format
                # prog = re.compile(filename_regex)
                match = re.match(filename_regex, filename)
                if match:
                    # Now match based on the current forecast hour
                    if file_type == 'FCST':
                        match_fhr = re.match(r'.*FCST_TILE_F([0-9]{3}).*',
                                             match.group())

                    elif file_type == 'ANLY':
                        match_fhr = re.match(r'.*ANLY_TILE_F([0-9]{3}).*',
                                             match.group())

                    if match_fhr:
                        if match_fhr.group(1) == cur_fhr_str:
                            # Join the two strings to form the full
                            # file path.
                            filepath = os.path.join(root, filename)
                            file_paths.append(filepath)
                else:
                    continue
        return file_paths

    def cleanup_lead_ascii(self):
        """! Remove any pre-existing FCST and ANLY ASCII files
            created by previous runs of series_by_lead.

            Args:

            Returns:
                None:    Removes any existing FCST and ANLY ASCII files
                         which contains all the forecast and analysis
                         gridded tiles.
        """

        for fhr in range(self.fhr_beg, self.fhr_end + 1, self.fhr_inc):
            cur_fhr = str(fhr).zfill(3)
            out_dir_parts = [self.series_lead_out_dir, '/', 'series_F',
                             cur_fhr]
            out_dir = ''.join(out_dir_parts)

            for root, directories, files in os.walk(out_dir):
                for cur_file in files:
                    fcst_match = re.match(self.fcst_ascii_regex, cur_file)
                    anly_match = re.match(self.anly_ascii_regex, cur_file)
                    rm_file = os.path.join(self.series_lead_out_dir, cur_file)
                    if fcst_match:
                        self.logger.debug("Cleaning up pre-existing "
                                          "forecast files")
                        os.remove(rm_file)
                    if anly_match:
                        os.remove(rm_file)

    def generate_plots(self, do_fhr_by_range):
        """! Generate the plots and animation GIFs for the series analysis
             results.

             Args:
                 @param do_fhr_by_range   The boolean flag which indicates
                                       whether series analysis is to be
                                       performed for the entire range of fhrs,
                                       (True), or by groups of fhrs (False).

             Returns: None

        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        # Generate a plot for each variable, statistic, and lead time.
        # First, retrieve all the netCDF files that were generated
        # above by the run series analysis.
        self.logger.info('GENERATING PLOTS...')

        # Retrieve a list of all the netCDF files generated by the
        # filtering.
        nc_list = self.retrieve_nc_files(do_fhr_by_range)

        # Check that we have netCDF files, if not, something went
        # wrong.
        if not nc_list:
            self.logger.error("ERROR|" + cur_filename + ":" + cur_function +
                              "]|  could not find any netCDF files to convert"
                              " to PS and PNG.  Exiting...")
            sys.exit(1)
        else:
            msg = ("INFO|[" + cur_filename + ":" + cur_function +
                   " Number of nc files found to convert to PS and PNG  : " +
                   str(len(nc_list)))
            self.logger.debug(msg)

        for cur_var in self.var_list:
            # Get the name and level to set the NAME and LEVEL
            # environment variables that
            # are needed by the MET series analysis binary.
            match = re.match(r'(.*)/(.*)', cur_var)
            name = match.group(1)
            level = match.group(2)

            os.environ['NAME'] = name
            os.environ['LEVEL'] = level

            if self.regrid_with_met_tool:
                os.environ['NAME'] = name + '_' + level

            # Retrieve only those netCDF files that correspond to
            # the current variable.
            nc_var_list = self.get_var_ncfiles(do_fhr_by_range, name, nc_list)
            if not nc_var_list:
                self.logger.debug("WARNING nc_var_list is empty for " + name +
                                  "_" + level + ", check for next variable...")
                continue

            # Iterate over the statistics, setting the CUR_STAT
            # environment variable...
            for cur_stat in self.stat_list:
                # Set environment variable required by MET
                # application Plot_Data_Plane.
                os.environ['CUR_STAT'] = cur_stat
                vmin, vmax = self.get_netcdf_min_max(do_fhr_by_range,
                                                     nc_var_list,
                                                     cur_stat)
                msg = ("|INFO|[ " + cur_filename + ":" + cur_function +
                       "]| Plotting range for " + cur_var + " " +
                       cur_stat + ":  " + str(vmin) + " to " + str(vmax))
                self.logger.debug(msg)

                # Plot the output for each time
                # DEBUG
                self.logger.info("Create PS and PNG")

                # pylint:disable=anomalous-backslash-in-string
                # This backslash is necessary in the regex.

                for cur_nc in nc_var_list:
                    # The postscript files are derived from
                    # each netCDF file. The postscript filename is
                    # created by replacing the '.nc' extension
                    # with '_<cur_stat>.ps'. The png file is created
                    # by replacing the '.ps'
                    # extension of the postscript file with '.png'.
                    repl_string = ['_', cur_stat, '.ps']
                    repl = ''.join(repl_string)
                    ps_file = re.sub('(\.nc)$', repl, cur_nc)

                    # Now create the PNG filename from the
                    # Postscript filename.
                    png_file = re.sub('(\.ps)$', '.png', ps_file)

                    # Extract the forecast hour from the netCDF
                    # filename.
                    if do_fhr_by_range:
                        match_fhr = re.match(
                            r'.*/series_F\d{3}/series_F(\d{3}).*\.nc',
                            cur_nc)
                    else:
                        match_fhr = re.match(
                            r'.*/.*/(series_F(\d{3})_'
                            r'to_F(\d{3})).*\.nc', cur_nc)

                    if match_fhr:
                        fhr = match_fhr.group(1)
                    else:
                        msg = ("WARNING: netCDF file format for file: " +
                               cur_nc +
                               " is unexpected. Try next file in list...")
                        self.logger.debug(msg)
                        continue

                    # Get the max series_cnt_TOTAL value (i.e. nseries)
                    nseries = self.get_nseries(do_fhr_by_range, cur_nc)

                    # Create the plot data plane command based on whether
                    # the background map was requested in the
                    # param/config file.
                    if self.background_map:
                        # Flag set to True, print background map.
                        map_data = ''
                    else:
                        map_data = "map_data={source=[];}  "

                    plot_data_plane_parts = [self.plot_data_plane_exe, ' ',
                                             cur_nc, ' ', ps_file, ' ',
                                             "'", 'name = ', '"',
                                             'series_cnt_', cur_stat, '";',
                                             'level=', '"(\*,\*)"; ',
                                             ' ', map_data,
                                             "'", ' -title ', '"GFS F',
                                             str(fhr),
                                             ' Forecasts (N = ', str(nseries),
                                             '), ', cur_stat, ' for ', cur_var,
                                             '"', ' -plot_range ', str(vmin),
                                             ' ', str(vmax)]

                    plot_data_plane_cmd = ''.join(plot_data_plane_parts)
                    plot_data_plane_cmd = \
                        batchexe('sh')['-c', plot_data_plane_cmd].err2out()
                    msg = ("INFO|[" + cur_filename + ":" +
                           cur_function + "]| plot_data_plane cmd: " +
                           plot_data_plane_cmd.to_shell())
                    self.logger.debug(msg)
                    run(plot_data_plane_cmd)

                    # Create the convert command.
                    convert_parts = [self.convert_exe, ' -rotate 90 ',
                                     ' -background white -flatten ',
                                     ps_file, ' ', png_file]
                    convert_cmd = ''.join(convert_parts)
                    convert_cmd = batchexe('sh')['-c', convert_cmd].err2out()
                    run(convert_cmd)

    def create_animated_gifs(self, do_fhr_by_range):
        """! Creates the animated GIF files from the .png files created in
             generate_plots().

             Args:
                  @param do_fhr_by_range:  The boolean flag indicating whether
                                        series analysis was performed on the
                                        entire range (True) or on groups of
                                        forecast hours (False).
            Returns:

        """

        # pylint:disable=protected-access
        # Need to call sys.__getframe() to get the filename and method/func
        # for logging information.
        cur_filename = sys._getframe().f_code.co_filename
        cur_function = sys._getframe().f_code.co_name

        animate_dir = os.path.join(self.series_lead_out_dir, 'series_animate')
        msg = ('INFO|[' + cur_filename + ':' + cur_function +
               ']| Creating Animation Plots, create directory:' +
               animate_dir)
        self.logger.debug(msg)
        util.mkdir_p(animate_dir)

        for cur_var in self.var_list:
            # Get the name and level to set the NAME and LEVEL
            # environment variables that
            # are needed by the MET series analysis binary.
            match = re.match(r'(.*)/(.*)', cur_var)
            name = match.group(1)
            level = match.group(2)

            os.environ['NAME'] = name
            os.environ['LEVEL'] = level

            if self.regrid_with_met_tool:
                os.environ['NAME'] = name + '_' + level

            self.logger.info("Creating animated gifs")
            for cur_stat in self.stat_list:
                if do_fhr_by_range:
                    series_dir = '/series_F*'
                    series_fname_root = '/series_F*'
                    gif_parts = [self.convert_exe,
                                 ' -dispose Background -delay 100 ',
                                 self.series_lead_out_dir, '/', series_dir,
                                 series_fname_root, '_', name, '_',
                                 level, '_', cur_stat, '.png', '  ',
                                 animate_dir, '/series_animate_', name, '_',
                                 level, '_', cur_stat, '.gif']
                    animate_cmd = ''.join(gif_parts)
                    self.logger.debug("animate cmd: {}".format(animate_cmd))
                    animate_cmd = batchexe('sh')['-c', animate_cmd].err2out()
                    msg = ("INFO|[" + cur_filename + ":" + cur_function +
                           "]| animate command: " + animate_cmd.to_shell())
                    self.logger.debug(msg)
                    run(animate_cmd)
                else:
                    # For series analysis by forecast hour groups, create a
                    # list of the series analysis output for all the forecast
                    # hour groups for each variable-level-statistic
                    # combination.
                    wildcard_list = []
                    for group_label in self.fhr_group_labels:
                        wildcard_parts = [self.series_lead_out_dir, '/',
                                          group_label, '/series_F*_to_F*_',
                                          name, '_', level, '_', cur_stat,
                                          '.png']
                        wildcard = ''.join(wildcard_parts)
                        wildcard_list.append(wildcard)
                    wildcard_string = ' '.join(wildcard_list)
                    self.logger.debug("DEBUG: By group wildcards: " +
                                      wildcard_string)

                    gif_parts = [self.convert_exe,
                                 ' -dispose Background -delay 100 ',
                                 wildcard_string, '  ', animate_dir,
                                 '/series_animate_', name, '_', level, '_',
                                 cur_stat, '.gif']

                    animate_cmd = ''.join(gif_parts)
                    self.logger.debug("animate cmd: {}".format(animate_cmd))
                    animate_cmd = batchexe('sh')['-c', animate_cmd].err2out()
                    msg = ("INFO|[" + cur_filename + ":" + cur_function +
                           "]| animate command: " + animate_cmd.to_shell())
                    self.logger.debug(msg)
                    run(animate_cmd)


if __name__ == "__main__":
    """! Set up the logging, configuration launcher, and environment
         variables.
    """
    try:
        if 'JLOGFILE' in os.environ:
            produtil.setup.setup(send_dbn=False, jobname='SeriesByLeadWrapper',
                                 jlogfile=os.environ['JLOGFILE'])
        else:
            produtil.setup.setup(send_dbn=False, jobname='SeriesByLeadWrapper')
        produtil.log.postmsg('SeriesByLeadWrapper is starting')

        # Read in the configuration object CONFIG
        CONFIG = config_metplus.setup()
        if 'MET_BASE' not in os.environ:
            os.environ['MET_BASE'] = CONFIG.getdir('MET_BASE')

        SBL = SeriesByLeadWrapper(CONFIG, logger=None)
        SBL.run_all_times()

        produtil.log.postmsg('series analysis by lead time completed')
    except Exception as e:
        produtil.log.jlogger.critical(
            'SeriesByLeadWrapper failed: %s' % (str(e),), exc_info=True)
        sys.exit(2)
